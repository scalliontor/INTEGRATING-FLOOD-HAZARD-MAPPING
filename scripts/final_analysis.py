import pandas as pd
import numpy as np
import xgboost as xgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pickle  # <--- THÆ¯ VIá»†N Äá»‚ LÆ¯U .PKL

# ==========================================================
# 1. LOAD Dá»® LIá»†U & Cáº¤U HÃŒNH
# ==========================================================
csv_path = 'HaTinh_Training_Ready_Clean.csv'
print(f"ðŸ”„ Äang Ä‘á»c dá»¯ liá»‡u: {csv_path}")
df = pd.read_csv(csv_path)

# Feature Set CHUáº¨N (ÄÃ£ bá» water_mask vÃ  elev do multicollinearity r=0.918 vá»›i relief)
features = [
    'slope', 'aspect', 'curv', 'relief', 'twi', 'flow_acc', 'dist_water', 
    'lulc', 'precip_clim', 
    'Rain_3D', 'Rain_7D', 'Rain_Max', 'Rain_AM14'
]
target = 'Label'

X = df[features]
y = df[target]

# ==========================================================
# 2. TRAIN MODEL FINAL
# ==========================================================
print("ðŸš€ Äang Train Model Final (GPU)...")

model = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=8,
    learning_rate=0.05,
    tree_method='hist',
    device='cuda', # GPU
    random_state=42
)

model.fit(X, y)

# ==========================================================
# 3. LÆ¯U MODEL (SAVE) - Cáº¢ 2 Äá»ŠNH Dáº NG
# ==========================================================
print("\nðŸ’¾ ÄANG LÆ¯U MODEL...")

# CÃ¡ch 1: LÆ°u dáº¡ng JSON (Chuáº©n cá»§a XGBoost - Nháº¹, tÆ°Æ¡ng thÃ­ch cao)
model.save_model('XGBoost_Flood_Model_Final.json')
print("   âœ… ÄÃ£ lÆ°u JSON: XGBoost_Flood_Model_Final.json")

# CÃ¡ch 2: LÆ°u dáº¡ng PICKLE (.pkl) - Tiá»‡n dá»¥ng cho Python
pkl_filename = "XGBoost_Flood_Model_Final.pkl"
with open(pkl_filename, "wb") as f:
    pickle.dump(model, f)
print(f"   âœ… ÄÃ£ lÆ°u PICKLE: {pkl_filename}")

# ==========================================================
# 4. SHAP ANALYSIS
# ==========================================================
print("\nâš¡ Äang tÃ­nh SHAP Values...")
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# A. SHAP SUMMARY
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, X, show=False)
plt.title('SHAP Summary (No Water Mask)', fontsize=14)
plt.tight_layout()
plt.savefig('Final_SHAP_Summary.png', dpi=300)
plt.close()
print("   âœ… ÄÃ£ lÆ°u áº£nh SHAP Summary")

# B. SHAP DEPENDENCE (MÆ¯A)
plt.figure(figsize=(10, 6))
shap.dependence_plot("Rain_7D", shap_values, X, interaction_index=None, show=False)
plt.title('Quan há»‡: MÆ°a 7 ngÃ y vs Nguy cÆ¡ Ngáº­p', fontsize=12)
plt.tight_layout()
plt.savefig('Final_SHAP_Rain7D.png', dpi=300)
plt.close()
print("   âœ… ÄÃ£ lÆ°u áº£nh SHAP Dependence")

# ==========================================================
# 5. Ká»ŠCH Báº¢N BÄKH (MÆ¯A TÄ‚NG 20%)
# ==========================================================
print("\nðŸŒ Äang cháº¡y mÃ´ phá»ng BÄKH (MÆ°a tÄƒng 20%)...")

X_scenario = X.copy()
rain_cols = ['Rain_3D', 'Rain_7D', 'Rain_Max', 'Rain_AM14']
for col in rain_cols:
    X_scenario[col] = X_scenario[col] * 1.2 

prob_baseline = model.predict_proba(X)[:, 1]
prob_scenario = model.predict_proba(X_scenario)[:, 1]

df['Risk_Increase'] = prob_scenario - prob_baseline
avg_increase = df.groupby('Event_Name')['Risk_Increase'].mean().sort_values(ascending=False)

print("\n--- TOP 5 Sá»° KIá»†N TÄ‚NG Rá»¦I RO Máº NH NHáº¤T ---")
print(avg_increase.head(5))

plt.figure(figsize=(14, 7))
sns.barplot(x=avg_increase.values, y=avg_increase.index, palette='Reds_r')
plt.title('Má»©c tÄƒng Nguy cÆ¡ Ngáº­p khi MÆ°a tÄƒng 20%', fontsize=14)
plt.xlabel('Delta Probability')
plt.tight_layout()
plt.savefig('Climate_Change_Impact.png', dpi=300)
plt.close()
print("   âœ… ÄÃ£ lÆ°u áº£nh BÄKH")

print("\nðŸŽ‰ HOÃ€N THÃ€NH! Báº N ÄÃƒ CÃ“ FILE .PKL Äá»‚ DÃ™NG SAU NÃ€Y.")